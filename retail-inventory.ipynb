{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b7f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pandas numpy Faker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483d690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from faker import Faker\n",
    "import random\n",
    "import os\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "transactions_rows = 220_000  # 1/10th of 2.2M\n",
    "items_rows = 500             # reasonable number for items\n",
    "cities_rows = 100            # cities metadata\n",
    "\n",
    "# --- Generate items.csv ---\n",
    "def generate_items(n):\n",
    "    categories = ['Stationery', 'Electronics', 'Home & Kitchen', 'Toys', 'Books', 'Clothing']\n",
    "    suppliers = [fake.company() for _ in range(50)]\n",
    "    data = {\n",
    "        'item_id': range(1000, 1000 + n),\n",
    "        'item_name': [fake.word().capitalize() for _ in range(n)],\n",
    "        'category': np.random.choice(categories, n),\n",
    "        'supplier': np.random.choice(suppliers, n),\n",
    "        'price_usd': np.round(np.random.uniform(1, 1000, n), 2),\n",
    "        'weight_kg': np.round(np.random.uniform(0.01, 10, n), 3),\n",
    "        'color': np.random.choice(['Red', 'Green', 'Blue', 'Black', 'White', 'Yellow'], n),\n",
    "        'warranty_years': np.random.choice([0,1,2,3,5], n),\n",
    "        'release_year': np.random.choice(range(2000, 2024), n),\n",
    "        'rating': np.round(np.random.uniform(1, 5, n), 1)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    # Introduce nulls randomly\n",
    "    for col in df.columns:\n",
    "        df.loc[df.sample(frac=0.05).index, col] = np.nan\n",
    "    # Introduce duplicates\n",
    "    df = pd.concat([df, df.sample(frac=0.02)], ignore_index=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# --- Generate cities.json ---\n",
    "def generate_cities(n):\n",
    "    states = list(set([fake.state_abbr() for _ in range(n*2)]))\n",
    "    countries = ['USA', 'Canada', 'Mexico']\n",
    "    cities = []\n",
    "    for i in range(n):\n",
    "        city = {\n",
    "            'city_id': 100 + i,\n",
    "            'city_name': fake.city(),\n",
    "            'state': np.random.choice(states),\n",
    "            'country': np.random.choice(countries),\n",
    "            'population': int(np.random.normal(500000, 300000)),\n",
    "            'area_sq_km': round(np.random.uniform(50, 5000), 2),\n",
    "            'average_income_usd': int(np.random.normal(50000, 15000)),\n",
    "            'founded_year': random.randint(1700, 2020),\n",
    "            'time_zone': fake.timezone(),\n",
    "            'climate': np.random.choice(['Temperate', 'Tropical', 'Arid', 'Continental', 'Polar'])\n",
    "        }\n",
    "        cities.append(city)\n",
    "    # Introduce nulls\n",
    "    for _ in range(5):\n",
    "        cities[random.randint(0, n-1)]['average_income_usd'] = None\n",
    "    # Introduce anomalies (location mismatch): change a city's state to wrong one randomly\n",
    "    for _ in range(3):\n",
    "        idx = random.randint(0, n-1)\n",
    "        cities[idx]['state'] = 'ZZ'  # invalid state code\n",
    "    # Introduce duplicates\n",
    "    cities.extend(random.sample(cities, 3))\n",
    "    return cities\n",
    "\n",
    "# --- Generate transactions.csv ---\n",
    "def generate_transactions(n, item_ids, city_ids):\n",
    "    payment_methods = ['Credit Card', 'Debit Card', 'Cash', 'Mobile Payment', 'Gift Card']\n",
    "    order_status = ['Completed', 'Pending', 'Cancelled', 'Returned']\n",
    "    data = {\n",
    "        'transaction_id': range(1, n + 1),\n",
    "        'item_id': np.random.choice(item_ids, n),\n",
    "        'city_id': np.random.choice(city_ids, n),\n",
    "        'quantity': np.random.poisson(2, n),\n",
    "        'price_usd': np.round(np.random.uniform(5, 500, n), 2),\n",
    "        'discount_percent': np.round(np.random.uniform(0, 30, n), 1),\n",
    "        'payment_method': np.random.choice(payment_methods, n),\n",
    "        'order_status': np.random.choice(order_status, n),\n",
    "        'customer_age': np.random.randint(18, 80, n),\n",
    "        'transaction_date': [fake.date_between(start_date='-1y', end_date='today').isoformat() for _ in range(n)]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    # Introduce nulls\n",
    "    for col in ['quantity', 'price_usd', 'discount_percent']:\n",
    "        df.loc[df.sample(frac=0.02).index, col] = np.nan\n",
    "    # Introduce duplicates (2% duplicates)\n",
    "    df = pd.concat([df, df.sample(frac=0.02)], ignore_index=True)\n",
    "    # Introduce anomalies (e.g., negative quantities or prices)\n",
    "    anomaly_indices = df.sample(frac=0.01).index\n",
    "    df.loc[anomaly_indices, 'quantity'] = -df.loc[anomaly_indices, 'quantity'].abs()\n",
    "    df.loc[anomaly_indices, 'price_usd'] = -df.loc[anomaly_indices, 'price_usd'].abs()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating items...\")\n",
    "items_df = generate_items(items_rows)\n",
    "\n",
    "print(\"Generating cities...\")\n",
    "cities_list = generate_cities(cities_rows)\n",
    "\n",
    "print(\"Generating transactions...\")\n",
    "transactions_df = generate_transactions(transactions_rows, items_df['item_id'].tolist(), [city['city_id'] for city in cities_list])\n",
    "\n",
    "# Save to files\n",
    "print(\"Saving datasets...\")\n",
    "\n",
    "items_df.to_csv('items.csv', index=False)\n",
    "with open('cities.json', 'w') as f:\n",
    "    json.dump(cities_list, f, indent=2)\n",
    "transactions_df.to_csv('transactions.csv', index=False)\n",
    "\n",
    "print(f\"Files saved in {os.getcwd()}\")\n",
    "print(\"Datasets generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c48b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2579d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:46:29 WARN Utils: Your hostname, user-virtual-machine resolves to a loopback address: 127.0.1.1; using 10.33.58.47 instead (on interface ens160)\n",
      "25/07/21 12:46:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 12:46:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/21 12:46:35 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/07/21 12:46:35 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/07/21 12:46:39 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/07/21 12:46:39 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore user@127.0.1.1\n",
      "25/07/21 12:46:39 WARN HadoopFSUtils: The directory file:/home/user/Documents/retail inventory optimisation/write_folder was not found. Was it deleted very recently?\n",
      "25/07/21 12:46:39 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/07/21 12:46:45 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/07/21 12:46:45 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/07/21 12:46:45 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/07/21 12:46:45 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Transactions Schema =====\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_usd: double (nullable = true)\n",
      " |-- discount_percent: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:46:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "|summary|    transaction_id|           item_id|           city_id|          quantity|         price_usd|  discount_percent|payment_method|order_status|      customer_age|transaction_date|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "|  count|            224400|            224400|            224400|            219902|            219924|            219927|        224400|      224400|            224400|          224400|\n",
      "|   mean|109971.21411319073|1251.3774286987523|149.53861408199643|1.9613191330683668|247.33349166075595|14.988835840983576|          NULL|        NULL| 48.49874331550802|            NULL|\n",
      "| stddev| 63512.13953268465|144.89527851151345|29.003435672097073|1.4701127700560135| 151.2549077844429| 8.644153037244982|          NULL|        NULL|17.881180840848184|            NULL|\n",
      "|    min|                 1|              1001|               100|                -9|           -499.85|               0.0|          Cash|   Cancelled|                18|      2024-07-20|\n",
      "|    max|            220000|              1499|               199|                11|             500.0|              30.0|Mobile Payment|    Returned|                79|      2025-07-20|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "\n",
      "===== Items Schema =====\n",
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- price_usd: double (nullable = true)\n",
      " |-- weight_kg: double (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- warranty_years: integer (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "|summary|           item_id|item_name|category|            supplier|        price_usd|        weight_kg| color|    warranty_years|     release_year|            rating|\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "|  count|               484|      485|     485|                 485|              485|              484|   483|               485|              485|               483|\n",
      "|   mean|1251.1921487603306|     NULL|    NULL|                NULL|501.2117525773202|5.149024793388429|  NULL|2.2783505154639174|2011.160824742268| 3.039130434782609|\n",
      "| stddev|145.13037709745657|     NULL|    NULL|                NULL|291.3840247561268|2.909921496064052|  NULL|1.7822719044376683|7.024375456797927|1.1699118975789153|\n",
      "|    min|              1001|  Ability|   Books|Alexander-Williamson|             2.04|            0.067| Black|                 0|             2000|               1.0|\n",
      "|    max|              1499| Yourself|    Toys| Yates, Lee and Pena|            999.6|            9.971|Yellow|                 5|             2023|               5.0|\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "\n",
      "===== Cities Schema =====\n",
      "root\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- area_sq_km: double (nullable = true)\n",
      " |-- average_income_usd: integer (nullable = true)\n",
      " |-- founded_year: integer (nullable = true)\n",
      " |-- time_zone: string (nullable = true)\n",
      " |-- climate: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "|summary|           city_id| city_name|state|country|       population|        area_sq_km|average_income_usd|     founded_year|           time_zone| climate|\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "|  count|               103|       103|  103|    103|              103|               103|                98|              103|                 103|     103|\n",
      "|   mean|149.53398058252426|      NULL| NULL|   NULL|542918.5922330098|2560.7314563106784| 49539.07142857143|1861.990291262136|                NULL|    NULL|\n",
      "| stddev| 29.13956125404174|      NULL| NULL|   NULL|293969.2633899139|1354.8999916298055|16226.614367766495|94.61329914660269|                NULL|    NULL|\n",
      "|    min|               100|Adamsville|   AK| Canada|           -77874|            131.68|              8967|             1709|  Africa/Addis_Ababa|    Arid|\n",
      "|    max|               199|  Woodbury|   ZZ|    USA|          1303549|           4988.56|             87126|             2015|Pacific/Port_Moresby|Tropical|\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Raw Ingestion - Sprint 1\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schemas\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"discount_percent\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "items_schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"item_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"supplier\", StringType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"weight_kg\", DoubleType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"warranty_years\", IntegerType(), True),\n",
    "    StructField(\"release_year\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "cities_schema = StructType([\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"population\", IntegerType(), True),\n",
    "    StructField(\"area_sq_km\", DoubleType(), True),\n",
    "    StructField(\"average_income_usd\", IntegerType(), True),\n",
    "    StructField(\"founded_year\", IntegerType(), True),\n",
    "    StructField(\"time_zone\", StringType(), True),\n",
    "    StructField(\"climate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load CSVs\n",
    "transactions_df = spark.read.csv(\n",
    "    \"/home/user/Documents/retail-inventory/retail-inventory-optimisation/transactions.csv\",\n",
    "    schema=transactions_schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "items_df = spark.read.csv(\n",
    "    \"/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv\",\n",
    "    schema=items_schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "cities_df = spark.read.json(  # assuming JSON for cities\n",
    "    \"/home/user/Documents/retail-inventory/retail-inventory-optimisation/cities.json\",\n",
    "    schema=cities_schema,\n",
    "    multiLine=True\n",
    ")\n",
    "\n",
    "# Register Hive External Tables\n",
    "transactions_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "    .saveAsTable(\"default.transactions_raw\")\n",
    "\n",
    "items_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "    .saveAsTable(\"default.items_raw\")\n",
    "\n",
    "cities_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "    .saveAsTable(\"default.cities_raw\")\n",
    "\n",
    "# Schema Validation Report\n",
    "print(\"===== Transactions Schema =====\")\n",
    "transactions_df.printSchema()\n",
    "transactions_df.describe().show()\n",
    "\n",
    "print(\"===== Items Schema =====\")\n",
    "items_df.printSchema()\n",
    "items_df.describe().show()\n",
    "\n",
    "print(\"===== Cities Schema =====\")\n",
    "cities_df.printSchema()\n",
    "cities_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2959db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:51:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CitiesCleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"cities.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b479a10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area_sq_km: double (nullable = true)\n",
      " |-- average_income_usd: long (nullable = true)\n",
      " |-- city_id: long (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- climate: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- founded_year: long (nullable = true)\n",
      " |-- population: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- time_zone: string (nullable = true)\n",
      "\n",
      "+----------+------------------+-------+------------------+-----------+-------+------------+----------+-----+------------------+\n",
      "|area_sq_km|average_income_usd|city_id|city_name         |climate    |country|founded_year|population|state|time_zone         |\n",
      "+----------+------------------+-------+------------------+-----------+-------+------------+----------+-----+------------------+\n",
      "|148.55    |38107             |100    |East Anthonyside  |Polar      |Canada |1996        |777286    |AK   |Africa/Bissau     |\n",
      "|830.61    |41816             |101    |Laurenburgh       |Arid       |Mexico |1949        |312031    |IN   |Africa/Djibouti   |\n",
      "|4020.3    |NULL              |102    |West Samanthaport |Temperate  |USA    |1851        |696601    |IN   |Asia/Phnom_Penh   |\n",
      "|1458.9    |52091             |103    |Marymouth         |Arid       |Mexico |1904        |200105    |VT   |America/Havana    |\n",
      "|3912.88   |42686             |104    |Susanside         |Continental|Mexico |1768        |606594    |HI   |Africa/Ouagadougou|\n",
      "|4524.16   |24741             |105    |Ramseybury        |Polar      |Canada |1985        |282192    |CO   |Asia/Seoul        |\n",
      "|1799.25   |33344             |106    |Randystad         |Arid       |Mexico |1766        |-26574    |MA   |Asia/Dili         |\n",
      "|3020.43   |49099             |107    |Adamsville        |Temperate  |Canada |1712        |837658    |ZZ   |Africa/Mbabane    |\n",
      "|1072.92   |33716             |108    |New Laurastad     |Arid       |Mexico |1891        |240735    |WV   |Africa/Maputo     |\n",
      "|3282.13   |60144             |109    |North Charlesland |Tropical   |USA    |1751        |212461    |KS   |America/Lima      |\n",
      "|1216.43   |67169             |110    |North Michaelstad |Continental|Canada |1819        |489070    |NV   |Africa/Nairobi    |\n",
      "|1784.86   |23611             |111    |East Stephaniebury|Continental|Mexico |1805        |519227    |LA   |Europe/Helsinki   |\n",
      "|131.68    |62121             |112    |North Andrewport  |Continental|Canada |1976        |680934    |WY   |Africa/Sao_Tome   |\n",
      "|2306.56   |8967              |113    |Lake Jessica      |Tropical   |Mexico |1992        |416375    |AR   |Asia/Bishkek      |\n",
      "|211.91    |49074             |114    |East Dannyburgh   |Arid       |Mexico |1817        |-77874    |AL   |Asia/Colombo      |\n",
      "|2991.42   |53495             |115    |Lake Taraburgh    |Tropical   |Canada |1791        |572522    |WI   |Africa/Kinshasa   |\n",
      "|4922.55   |70177             |116    |Michaelchester    |Continental|USA    |1709        |693964    |WI   |Europe/Stockholm  |\n",
      "|2930.34   |45856             |117    |Smithfort         |Polar      |Canada |1913        |764230    |WY   |Africa/Monrovia   |\n",
      "|974.35    |75484             |118    |Samanthaville     |Polar      |USA    |1970        |677325    |PR   |Indian/Maldives   |\n",
      "|4232.52   |55597             |119    |East Whitney      |Arid       |Canada |1894        |196095    |MP   |Europe/Prague     |\n",
      "+----------+------------------+-------+------------------+-----------+-------+------------+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e304347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c277ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "cleaned_df=df\n",
    "\n",
    "# Remove duplicate rows based on 'city_id'\n",
    "cleaned_df = cleaned_df.dropDuplicates(['city_id'])\n",
    "\n",
    "# Impute missing values in 'average_income_usd' with the mean\n",
    "# Calculate the mean of 'average_income_usd' (excluding nulls).\n",
    "mean_income = cleaned_df.agg(mean('average_income_usd')).first()[0]\n",
    "# Fill all null values in 'average_income_usd' with the calculated mean.\n",
    "cleaned_df = cleaned_df.fillna({'average_income_usd': mean_income})\n",
    "\n",
    "# Remove rows where the 'population' is negative\n",
    "# Ensures all cities have non-negative population counts.\n",
    "cleaned_df = cleaned_df.filter(col('population') >= 0)\n",
    "\n",
    "# Standardize 'city_name' by removing leading/trailing spaces\n",
    "# Ensures city names are cleaned of any unwanted space characters.\n",
    "cleaned_df = cleaned_df.withColumn('city_name', trim(col('city_name')))\n",
    "\n",
    "# Filter cities by 'founded_year' to keep only reasonable years (between 1000 and 2025)\n",
    "# Removes cities with obviously invalid foundation years.\n",
    "cleaned_df = cleaned_df.filter((col('founded_year') >= 1000) & (col('founded_year') <= 2025))\n",
    "\n",
    "# Remove rows where 'state' or 'country' is missing (null)\n",
    "# Ensures all cities have both a state and a country specified.\n",
    "cleaned_df = cleaned_df.filter(col('state').isNotNull() & col('country').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d4831b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4eb47c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
