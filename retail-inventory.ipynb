{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b7f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b89ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4c48b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark==3.5.2\n",
      "  Using cached pyspark-3.5.2.tar.gz (317.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /home/user/.local/lib/python3.10/site-packages (from pyspark==3.5.2) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "\u001b[33m  DEPRECATION: Building 'pyspark' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyspark'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812388 sha256=9fff3b35ad735b49842de44f03d63aa449e1bab726f78e9a3d0273f33c90fd8c\n",
      "  Stored in directory: /home/user/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
      "Successfully built pyspark\n",
      "\u001b[33mWARNING: Error parsing dependencies of send2trash: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    sys-platform (==\"darwin\") ; extra == 'objc'\n",
      "                 ~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.4\n",
      "    Uninstalling pyspark-3.5.4:\n",
      "      Successfully uninstalled pyspark-3.5.4\n",
      "Successfully installed pyspark-3.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark==3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2579d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 09:52:51 WARN Utils: Your hostname, user-virtual-machine resolves to a loopback address: 127.0.1.1; using 10.33.60.39 instead (on interface ens160)\n",
      "25/07/22 09:52:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/22 09:52:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Transactions Schema =====\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_usd: double (nullable = true)\n",
      " |-- discount_percent: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 09:52:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "|summary|    transaction_id|           item_id|           city_id|          quantity|         price_usd|  discount_percent|payment_method|order_status|      customer_age|transaction_date|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "|  count|            224400|            224400|            224400|            219902|            219924|            219927|        224400|      224400|            224400|          224400|\n",
      "|   mean|109971.21411319073|1251.3774286987523|149.53861408199643|1.9613191330683668|247.33349166075595|14.988835840983576|          null|        null| 48.49874331550802|            null|\n",
      "| stddev| 63512.13953268465|144.89527851151345|29.003435672097073|1.4701127700560135| 151.2549077844429| 8.644153037244982|          null|        null|17.881180840848184|            null|\n",
      "|    min|                 1|              1001|               100|                -9|           -499.85|               0.0|          Cash|   Cancelled|                18|      2024-07-20|\n",
      "|    max|            220000|              1499|               199|                11|             500.0|              30.0|Mobile Payment|    Returned|                79|      2025-07-20|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "\n",
      "===== Items Schema =====\n",
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- price_usd: double (nullable = true)\n",
      " |-- weight_kg: double (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- warranty_years: integer (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "|summary|           item_id|item_name|category|            supplier|        price_usd|        weight_kg| color|    warranty_years|     release_year|            rating|\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "|  count|               484|      485|     485|                 485|              485|              484|   483|               485|              485|               483|\n",
      "|   mean|1251.1921487603306|     null|    null|                null|501.2117525773202|5.149024793388429|  null|2.2783505154639174|2011.160824742268| 3.039130434782609|\n",
      "| stddev|145.13037709745657|     null|    null|                null|291.3840247561268|2.909921496064052|  null|1.7822719044376683|7.024375456797927|1.1699118975789153|\n",
      "|    min|              1001|  Ability|   Books|Alexander-Williamson|             2.04|            0.067| Black|                 0|             2000|               1.0|\n",
      "|    max|              1499| Yourself|    Toys| Yates, Lee and Pena|            999.6|            9.971|Yellow|                 5|             2023|               5.0|\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "\n",
      "===== Cities Schema =====\n",
      "root\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- area_sq_km: double (nullable = true)\n",
      " |-- average_income_usd: integer (nullable = true)\n",
      " |-- founded_year: integer (nullable = true)\n",
      " |-- time_zone: string (nullable = true)\n",
      " |-- climate: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "|summary|           city_id| city_name|state|country|       population|        area_sq_km|average_income_usd|     founded_year|           time_zone| climate|\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "|  count|               103|       103|  103|    103|              103|               103|                98|              103|                 103|     103|\n",
      "|   mean|149.53398058252426|      null| null|   null|542918.5922330098|2560.7314563106784| 49539.07142857143|1861.990291262136|                null|    null|\n",
      "| stddev| 29.13956125404174|      null| null|   null|293969.2633899139|1354.8999916298055|16226.614367766495|94.61329914660269|                null|    null|\n",
      "|    min|               100|Adamsville|   AK| Canada|           -77874|            131.68|              8967|             1709|  Africa/Addis_Ababa|    Arid|\n",
      "|    max|               199|  Woodbury|   ZZ|    USA|          1303549|           4988.56|             87126|             2015|Pacific/Port_Moresby|Tropical|\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailInventory\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schemas\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"discount_percent\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "items_schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"item_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"supplier\", StringType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"weight_kg\", DoubleType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"warranty_years\", IntegerType(), True),\n",
    "    StructField(\"release_year\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "cities_schema = StructType([\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"population\", IntegerType(), True),\n",
    "    StructField(\"area_sq_km\", DoubleType(), True),\n",
    "    StructField(\"average_income_usd\", IntegerType(), True),\n",
    "    StructField(\"founded_year\", IntegerType(), True),\n",
    "    StructField(\"time_zone\", StringType(), True),\n",
    "    StructField(\"climate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load CSVs\n",
    "transactions_df = spark.read.csv(\n",
    "    \"file:///home/user/Documents/retail inventory optimisation/Raw_Data/Transaction_raw_data/transactions.csv\",\n",
    "    schema=transactions_schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "items_df = spark.read.csv(\n",
    "    \"file:///home/user/Documents/retail inventory optimisation/Raw_Data/items.csv\",\n",
    "    schema=items_schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "cities_df = spark.read.json(  # assuming JSON for cities\n",
    "    \"file:///home/user/Documents/retail inventory optimisation/Raw_Data/cities.json\",\n",
    "    schema=cities_schema,\n",
    "    multiLine=True\n",
    ")\n",
    "\n",
    "# # Register Hive External Tables\n",
    "# transactions_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "#     .saveAsTable(\"default.transactions_raw\")\n",
    "\n",
    "# items_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "#     .saveAsTable(\"default.items_raw\")\n",
    "\n",
    "\n",
    "# cities_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "#     .saveAsTable(\"default.cities_raw\")\n",
    "\n",
    "# Schema Validation Report\n",
    "print(\"===== Transactions Schema =====\")\n",
    "transactions_df.printSchema()\n",
    "transactions_df.describe().show()\n",
    "\n",
    "print(\"===== Items Schema =====\")\n",
    "items_df.printSchema()\n",
    "items_df.describe().show()\n",
    "\n",
    "print(\"===== Cities Schema =====\")\n",
    "cities_df.printSchema()\n",
    "cities_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1125f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before cleaning: 510\n",
      "+-------+---------+--------+--------+---------+---------+-----+--------------+------------+------+\n",
      "|item_id|item_name|category|supplier|price_usd|weight_kg|color|warranty_years|release_year|rating|\n",
      "+-------+---------+--------+--------+---------+---------+-----+--------------+------------+------+\n",
      "|     26|       25|      25|      25|       25|       26|   27|            25|          25|    27|\n",
      "+-------+---------+--------+--------+---------+---------+-----+--------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Total records before cleaning:\", items_df.count())\n",
    "# Check nulls\n",
    "items_df.select([count(when(col(c).isNull(), c)).alias(c) for c in items_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e9ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after cleaning: 354\n",
      "+-------+------------------+-----------------+-----------------+------------------+\n",
      "|summary|         price_usd|        weight_kg|         item_age|            rating|\n",
      "+-------+------------------+-----------------+-----------------+------------------+\n",
      "|  count|               354|              354|              354|               340|\n",
      "|   mean| 499.6977118644069|5.052802259887001|14.17231638418079| 2.976764705882352|\n",
      "| stddev|287.11154156305804|2.940548365809729|7.082374223909156|1.1737121118334402|\n",
      "|    min|              2.04|            0.067|                2|               1.0|\n",
      "|    max|             999.6|            9.971|               25|               5.0|\n",
      "+-------+------------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Removing duplicate item_id\n",
    "cleaned_items_df = items_df.dropDuplicates([\"item_id\"])\n",
    "\n",
    "# Drop rows with nulls in critical columns\n",
    "required_columns = [\"item_id\", \"item_name\", \"price_usd\", \"weight_kg\", \"category\", \"release_year\"]\n",
    "cleaned_items_df = cleaned_items_df.dropna(subset=required_columns)\n",
    "\n",
    "# Filter invalid prices and weights\n",
    "cleaned_items_df = cleaned_items_df.filter((col(\"price_usd\") > 0) & (col(\"weight_kg\") > 0))\n",
    "\n",
    "# Filter warranty (valid in [0-5])\n",
    "cleaned_items_df = cleaned_items_df.filter((col(\"warranty_years\") >= 0) & (col(\"warranty_years\") <= 5))\n",
    "\n",
    "# Filter based on acceptable release years (2000 to current year)\n",
    "current_year = datetime.datetime.now().year\n",
    "cleaned_items_df = cleaned_items_df.filter((col(\"release_year\") >= 2000) & (col(\"release_year\") <= current_year))\n",
    "\n",
    "# Add derived column: item_age\n",
    "cleaned_items_df = cleaned_items_df.withColumn(\"item_age\", lit(current_year) - col(\"release_year\"))\n",
    "\n",
    "print(\"Total records after cleaning:\", cleaned_items_df.count())\n",
    "cleaned_items_df.describe([\"price_usd\", \"weight_kg\", \"item_age\", \"rating\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243fea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "cleaned_df=cities_df\n",
    "\n",
    "# Remove duplicate rows based on 'city_id'\n",
    "cleaned_df = cleaned_df.dropDuplicates(['city_id'])\n",
    "\n",
    "# Impute missing values in 'average_income_usd' with the mean\n",
    "# Calculate the mean of 'average_income_usd' (excluding nulls).\n",
    "mean_income = cleaned_df.agg(mean('average_income_usd')).first()[0]\n",
    "# Fill all null values in 'average_income_usd' with the calculated mean.\n",
    "cleaned_df = cleaned_df.fillna({'average_income_usd': mean_income})\n",
    "\n",
    "# Remove rows where the 'population' is negative\n",
    "# Ensures all cities have non-negative population counts.\n",
    "cleaned_df = cleaned_df.filter(col('population') >= 0)\n",
    "\n",
    "# Standardize 'city_name' by removing leading/trailing spaces\n",
    "# Ensures city names are cleaned of any unwanted space characters.\n",
    "cleaned_df = cleaned_df.withColumn('city_name', trim(col('city_name')))\n",
    "\n",
    "# Filter cities by 'founded_year' to keep only reasonable years (between 1000 and 2025)\n",
    "# Removes cities with obviously invalid foundation years.\n",
    "cleaned_df = cleaned_df.filter((col('founded_year') >= 1000) & (col('founded_year') <= 2025))\n",
    "\n",
    "# Remove rows where 'state' or 'country' is missing (null)\n",
    "# Ensures all cities have both a state and a country specified.\n",
    "cleaned_df = cleaned_df.filter(col('state').isNotNull() & col('country').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b5b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
