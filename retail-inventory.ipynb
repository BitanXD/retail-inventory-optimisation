{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980b7f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (1.21.5)\n",
      "Collecting Faker\n",
      "  Downloading faker-37.4.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Downloading faker-37.4.2-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of send2trash: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    sys-platform (==\"darwin\") ; extra == 'objc'\n",
      "                 ~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tzdata, python-dateutil, numpy, pandas, Faker\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [Faker]━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [Faker]]dateutil]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Faker-37.4.2 numpy-2.2.6 pandas-2.3.1 python-dateutil-2.9.0.post0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy Faker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629b89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating items...\n",
      "Generating cities...\n",
      "Generating transactions...\n",
      "Saving datasets...\n",
      "Files saved in /home/user/Documents/retail inventory optimisation\n",
      "Datasets generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from faker import Faker\n",
    "import random\n",
    "import os\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "transactions_rows = 220_000  # 1/10th of 2.2M\n",
    "items_rows = 500             # reasonable number for items\n",
    "cities_rows = 100            # cities metadata\n",
    "\n",
    "# --- Generate items.csv ---\n",
    "def generate_items(n):\n",
    "    categories = ['Stationery', 'Electronics', 'Home & Kitchen', 'Toys', 'Books', 'Clothing']\n",
    "    suppliers = [fake.company() for _ in range(50)]\n",
    "    data = {\n",
    "        'item_id': list(range(1000, 1000 + n)),\n",
    "        'item_name': [fake.word().capitalize() for _ in range(n)],\n",
    "        'category': np.random.choice(categories, n),\n",
    "        'supplier': np.random.choice(suppliers, n),\n",
    "        'price_usd': np.round(np.random.uniform(1, 1000, n), 2),\n",
    "        'weight_kg': np.round(np.random.uniform(0.01, 10, n), 3),\n",
    "        'color': np.random.choice(['Red', 'Green', 'Blue', 'Black', 'White', 'Yellow'], n),\n",
    "        'warranty_years': np.random.choice([0, 1, 2, 3, 5], n),\n",
    "        'release_year': np.random.choice(range(2000, 2024), n),\n",
    "        'rating': np.round(np.random.uniform(1, 5, n), 1)\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Introduce nulls randomly\n",
    "    for col in df.columns:\n",
    "        df.loc[df.sample(frac=0.05).index, col] = np.nan\n",
    "\n",
    "    # Introduce duplicates\n",
    "    df = pd.concat([df, df.sample(frac=0.02)], ignore_index=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Type enforcement\n",
    "    df['item_id'] = df['item_id'].astype('Int64')\n",
    "    df['release_year'] = df['release_year'].astype('Int64')\n",
    "    df['warranty_years'] = df['warranty_years'].astype('Int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Generate cities.json ---\n",
    "def generate_cities(n):\n",
    "    states = list(set([fake.state_abbr() for _ in range(n * 2)]))\n",
    "    countries = ['USA', 'Canada', 'Mexico']\n",
    "    cities = []\n",
    "\n",
    "    for i in range(n):\n",
    "        city = {\n",
    "            'city_id': int(100 + i),\n",
    "            'city_name': fake.city(),\n",
    "            'state': np.random.choice(states),\n",
    "            'country': np.random.choice(countries),\n",
    "            'population': int(np.random.normal(500000, 300000)),\n",
    "            'area_sq_km': round(np.random.uniform(50, 5000), 2),\n",
    "            'average_income_usd': int(np.random.normal(50000, 15000)),\n",
    "            'founded_year': random.randint(1700, 2020),\n",
    "            'time_zone': fake.timezone(),\n",
    "            'climate': np.random.choice(['Temperate', 'Tropical', 'Arid', 'Continental', 'Polar'])\n",
    "        }\n",
    "        cities.append(city)\n",
    "\n",
    "    # Introduce nulls\n",
    "    for _ in range(5):\n",
    "        cities[random.randint(0, n - 1)]['average_income_usd'] = None\n",
    "\n",
    "    # Introduce anomalies (invalid states)\n",
    "    for _ in range(3):\n",
    "        idx = random.randint(0, n - 1)\n",
    "        cities[idx]['state'] = 'ZZ'  # invalid state code\n",
    "\n",
    "    # Introduce duplicates\n",
    "    cities.extend(random.sample(cities, 3))\n",
    "\n",
    "    return cities\n",
    "\n",
    "# --- Generate transactions.csv ---\n",
    "def generate_transactions(n, item_ids, city_ids):\n",
    "    payment_methods = ['Credit Card', 'Debit Card', 'Cash', 'Mobile Payment', 'Gift Card']\n",
    "    order_statuses = ['Completed', 'Pending', 'Cancelled', 'Returned']\n",
    "\n",
    "    data = {\n",
    "        'transaction_id': list(range(1, n + 1)),\n",
    "        'item_id': np.random.choice(item_ids, n),\n",
    "        'city_id': np.random.choice(city_ids, n),\n",
    "        'quantity': np.random.poisson(2, n),\n",
    "        'price_usd': np.round(np.random.uniform(5, 500, n), 2),\n",
    "        'discount_percent': np.round(np.random.uniform(0, 30, n), 1),\n",
    "        'payment_method': np.random.choice(payment_methods, n),\n",
    "        'order_status': np.random.choice(order_statuses, n),\n",
    "        'customer_age': np.random.randint(18, 80, n),\n",
    "        'transaction_date': [fake.date_between(start_date='-1y', end_date='today').isoformat() for _ in range(n)]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Introduce nulls\n",
    "    for col in ['quantity', 'price_usd', 'discount_percent']:\n",
    "        df.loc[df.sample(frac=0.02).index, col] = np.nan\n",
    "\n",
    "    # Introduce duplicates\n",
    "    df = pd.concat([df, df.sample(frac=0.02)], ignore_index=True)\n",
    "\n",
    "    # Introduce anomalies: negative prices and quantities\n",
    "    anomaly_indices = df.sample(frac=0.01).index\n",
    "    df.loc[anomaly_indices, 'quantity'] = -df.loc[anomaly_indices, 'quantity'].abs()\n",
    "    df.loc[anomaly_indices, 'price_usd'] = -df.loc[anomaly_indices, 'price_usd'].abs()\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Type enforcement\n",
    "    df['transaction_id'] = df['transaction_id'].astype('Int64')\n",
    "    df['item_id'] = df['item_id'].astype('Int64')\n",
    "    df['city_id'] = df['city_id'].astype('Int64')\n",
    "    df['quantity'] = df['quantity'].astype('Int64')\n",
    "    df['customer_age'] = df['customer_age'].astype('Int64')\n",
    "    df['transaction_date'] = df['transaction_date'].astype('string')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate datasets\n",
    "print(\"Generating items...\")\n",
    "items_df = generate_items(items_rows)\n",
    "\n",
    "print(\"Generating cities...\")\n",
    "cities_list = generate_cities(cities_rows)\n",
    "city_ids = [city['city_id'] for city in cities_list]\n",
    "\n",
    "print(\"Generating transactions...\")\n",
    "transactions_df = generate_transactions(transactions_rows, items_df['item_id'].dropna().astype(int).tolist(), city_ids)\n",
    "\n",
    "# Save to files\n",
    "print(\"Saving datasets...\")\n",
    "items_df.to_csv('items.csv', index=False)\n",
    "with open('cities.json', 'w') as f:\n",
    "    json.dump(cities_list, f, indent=2)\n",
    "transactions_df.to_csv('transactions.csv', index=False)\n",
    "\n",
    "print(f\"Files saved in {os.getcwd()}\")\n",
    "print(\"Datasets generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4c48b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/user/.local/lib/python3.10/site-packages (3.5.4)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/user/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "\u001b[33mWARNING: Error parsing dependencies of send2trash: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    sys-platform (==\"darwin\") ; extra == 'objc'\n",
      "                 ~^\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2579d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:19:37 WARN Utils: Your hostname, user-virtual-machine resolves to a loopback address: 127.0.1.1; using 10.33.60.39 instead (on interface ens160)\n",
      "25/07/21 12:19:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 12:19:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/21 12:19:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/21 12:19:44 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/07/21 12:19:44 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/07/21 12:19:47 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/07/21 12:19:47 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore user@127.0.1.1\n",
      "25/07/21 12:19:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/07/21 12:19:53 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/07/21 12:19:53 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/07/21 12:19:53 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/07/21 12:19:53 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Transactions Schema =====\n",
      "root\n",
      " |-- transaction_id: integer (nullable = true)\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price_usd: double (nullable = true)\n",
      " |-- discount_percent: double (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- customer_age: integer (nullable = true)\n",
      " |-- transaction_date: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 12:19:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "|summary|    transaction_id|           item_id|           city_id|          quantity|         price_usd|  discount_percent|payment_method|order_status|      customer_age|transaction_date|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "|  count|            224400|            224400|            224400|            219902|            219924|            219927|        224400|      224400|            224400|          224400|\n",
      "|   mean|109971.21411319073|1251.3774286987523|149.53861408199643|1.9613191330683668|247.33349166075595|14.988835840983576|          NULL|        NULL| 48.49874331550802|            NULL|\n",
      "| stddev| 63512.13953268465|144.89527851151345|29.003435672097073|1.4701127700560135| 151.2549077844429| 8.644153037244982|          NULL|        NULL|17.881180840848184|            NULL|\n",
      "|    min|                 1|              1001|               100|                -9|           -499.85|               0.0|          Cash|   Cancelled|                18|      2024-07-20|\n",
      "|    max|            220000|              1499|               199|                11|             500.0|              30.0|Mobile Payment|    Returned|                79|      2025-07-20|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+--------------+------------+------------------+----------------+\n",
      "\n",
      "===== Items Schema =====\n",
      "root\n",
      " |-- item_id: integer (nullable = true)\n",
      " |-- item_name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- price_usd: double (nullable = true)\n",
      " |-- weight_kg: double (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      " |-- warranty_years: integer (nullable = true)\n",
      " |-- release_year: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      "\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "|summary|           item_id|item_name|category|            supplier|        price_usd|        weight_kg| color|    warranty_years|     release_year|            rating|\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "|  count|               484|      485|     485|                 485|              485|              484|   483|               485|              485|               483|\n",
      "|   mean|1251.1921487603306|     NULL|    NULL|                NULL|501.2117525773202|5.149024793388429|  NULL|2.2783505154639174|2011.160824742268| 3.039130434782609|\n",
      "| stddev|145.13037709745657|     NULL|    NULL|                NULL|291.3840247561268|2.909921496064052|  NULL|1.7822719044376683|7.024375456797927|1.1699118975789153|\n",
      "|    min|              1001|  Ability|   Books|Alexander-Williamson|             2.04|            0.067| Black|                 0|             2000|               1.0|\n",
      "|    max|              1499| Yourself|    Toys| Yates, Lee and Pena|            999.6|            9.971|Yellow|                 5|             2023|               5.0|\n",
      "+-------+------------------+---------+--------+--------------------+-----------------+-----------------+------+------------------+-----------------+------------------+\n",
      "\n",
      "===== Cities Schema =====\n",
      "root\n",
      " |-- city_id: integer (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- population: integer (nullable = true)\n",
      " |-- area_sq_km: double (nullable = true)\n",
      " |-- average_income_usd: integer (nullable = true)\n",
      " |-- founded_year: integer (nullable = true)\n",
      " |-- time_zone: string (nullable = true)\n",
      " |-- climate: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "|summary|           city_id| city_name|state|country|       population|        area_sq_km|average_income_usd|     founded_year|           time_zone| climate|\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "|  count|               103|       103|  103|    103|              103|               103|                98|              103|                 103|     103|\n",
      "|   mean|149.53398058252426|      NULL| NULL|   NULL|542918.5922330098|2560.7314563106784| 49539.07142857143|1861.990291262136|                NULL|    NULL|\n",
      "| stddev| 29.13956125404174|      NULL| NULL|   NULL|293969.2633899139|1354.8999916298055|16226.614367766495|94.61329914660269|                NULL|    NULL|\n",
      "|    min|               100|Adamsville|   AK| Canada|           -77874|            131.68|              8967|             1709|  Africa/Addis_Ababa|    Arid|\n",
      "|    max|               199|  Woodbury|   ZZ|    USA|          1303549|           4988.56|             87126|             2015|Pacific/Port_Moresby|Tropical|\n",
      "+-------+------------------+----------+-----+-------+-----------------+------------------+------------------+-----------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RetailInventory\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schemas\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"discount_percent\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"customer_age\", IntegerType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "items_schema = StructType([\n",
    "    StructField(\"item_id\", IntegerType(), True),\n",
    "    StructField(\"item_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"supplier\", StringType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"weight_kg\", DoubleType(), True),\n",
    "    StructField(\"color\", StringType(), True),\n",
    "    StructField(\"warranty_years\", IntegerType(), True),\n",
    "    StructField(\"release_year\", IntegerType(), True),\n",
    "    StructField(\"rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "cities_schema = StructType([\n",
    "    StructField(\"city_id\", IntegerType(), True),\n",
    "    StructField(\"city_name\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"population\", IntegerType(), True),\n",
    "    StructField(\"area_sq_km\", DoubleType(), True),\n",
    "    StructField(\"average_income_usd\", IntegerType(), True),\n",
    "    StructField(\"founded_year\", IntegerType(), True),\n",
    "    StructField(\"time_zone\", StringType(), True),\n",
    "    StructField(\"climate\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load CSVs\n",
    "transactions_df = spark.read.csv(\n",
    "    \"/home/user/Documents/retail inventory optimisation/transactions.csv\",\n",
    "    schema=transactions_schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "items_df = spark.read.csv(\n",
    "    \"/home/user/Documents/retail inventory optimisation/items.csv\",\n",
    "    schema=items_schema,\n",
    "    header=True,\n",
    "    sep=\",\"\n",
    ")\n",
    "\n",
    "cities_df = spark.read.json(  # assuming JSON for cities\n",
    "    \"/home/user/Documents/retail inventory optimisation/cities.json\",\n",
    "    schema=cities_schema,\n",
    "    multiLine=True\n",
    ")\n",
    "\n",
    "# Register Hive External Tables\n",
    "transactions_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "    .saveAsTable(\"default.transactions_raw\")\n",
    "\n",
    "items_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "    .saveAsTable(\"default.items_raw\")\n",
    "\n",
    "\n",
    "cities_df.write.mode(\"overwrite\").option(\"path\", \"/home/user/Documents/retail inventory optimisation/write_folder/\") \\\n",
    "    .saveAsTable(\"default.cities_raw\")\n",
    "\n",
    "# Schema Validation Report\n",
    "print(\"===== Transactions Schema =====\")\n",
    "transactions_df.printSchema()\n",
    "transactions_df.describe().show()\n",
    "\n",
    "print(\"===== Items Schema =====\")\n",
    "items_df.printSchema()\n",
    "items_df.describe().show()\n",
    "\n",
    "print(\"===== Cities Schema =====\")\n",
    "cities_df.printSchema()\n",
    "cities_df.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1125f6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before cleaning: 510\n",
      "+-------+---------+--------+--------+---------+---------+-----+--------------+------------+------+\n",
      "|item_id|item_name|category|supplier|price_usd|weight_kg|color|warranty_years|release_year|rating|\n",
      "+-------+---------+--------+--------+---------+---------+-----+--------------+------------+------+\n",
      "|     26|       25|      25|      25|       25|       26|   27|            25|          25|    27|\n",
      "+-------+---------+--------+--------+---------+---------+-----+--------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "print(\"Total records before cleaning:\", items_df.count())\n",
    "# Check nulls\n",
    "items_df.select([count(when(col(c).isNull(), c)).alias(c) for c in items_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e9ca9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 15:07:12 ERROR Executor: Exception in task 0.0 in stage 47.0 (TID 35)\n",
      "org.apache.spark.SparkFileNotFoundException: File file:/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/07/21 15:07:12 WARN TaskSetManager: Lost task 0.0 in stage 47.0 (TID 35) (10.33.58.47 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv does not exist\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n",
      "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/07/21 15:07:12 ERROR TaskSetManager: Task 0 in stage 47.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o224.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 35) (10.33.58.47 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_307471/2446428353.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcleaned_items_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_items_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"item_age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_year\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"release_year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total records after cleaning:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaned_items_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mcleaned_items_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"price_usd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"weight_kg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"item_age\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \"\"\"\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o224.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 35) (10.33.58.47 executor driver): org.apache.spark.SparkFileNotFoundException: File file:/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/home/user/Documents/retail-inventory/retail-inventory-optimisation/items.csv does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:880)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:880)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Removing duplicate item_id\n",
    "cleaned_items_df = items_df.dropDuplicates([\"item_id\"])\n",
    "\n",
    "# Drop rows with nulls in critical columns\n",
    "required_columns = [\"item_id\", \"item_name\", \"price_usd\", \"weight_kg\", \"category\", \"release_year\"]\n",
    "cleaned_items_df = cleaned_items_df.dropna(subset=required_columns)\n",
    "\n",
    "# Filter invalid prices and weights\n",
    "cleaned_items_df = cleaned_items_df.filter((col(\"price_usd\") > 0) & (col(\"weight_kg\") > 0))\n",
    "\n",
    "# Filter warranty (valid in [0-5])\n",
    "cleaned_items_df = cleaned_items_df.filter((col(\"warranty_years\") >= 0) & (col(\"warranty_years\") <= 5))\n",
    "\n",
    "# Filter based on acceptable release years (2000 to current year)\n",
    "current_year = datetime.datetime.now().year\n",
    "cleaned_items_df = cleaned_items_df.filter((col(\"release_year\") >= 2000) & (col(\"release_year\") <= current_year))\n",
    "\n",
    "# Add derived column: item_age\n",
    "cleaned_items_df = cleaned_items_df.withColumn(\"item_age\", lit(current_year) - col(\"release_year\"))\n",
    "\n",
    "print(\"Total records after cleaning:\", cleaned_items_df.count())\n",
    "cleaned_items_df.describe([\"price_usd\", \"weight_kg\", \"item_age\", \"rating\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CitiesCleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"cities.json\")\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "cleaned_df=df\n",
    "\n",
    "# Remove duplicate rows based on 'city_id'\n",
    "cleaned_df = cleaned_df.dropDuplicates(['city_id'])\n",
    "\n",
    "# Impute missing values in 'average_income_usd' with the mean\n",
    "# Calculate the mean of 'average_income_usd' (excluding nulls).\n",
    "mean_income = cleaned_df.agg(mean('average_income_usd')).first()[0]\n",
    "# Fill all null values in 'average_income_usd' with the calculated mean.\n",
    "cleaned_df = cleaned_df.fillna({'average_income_usd': mean_income})\n",
    "\n",
    "# Remove rows where the 'population' is negative\n",
    "# Ensures all cities have non-negative population counts.\n",
    "cleaned_df = cleaned_df.filter(col('population') >= 0)\n",
    "\n",
    "# Standardize 'city_name' by removing leading/trailing spaces\n",
    "# Ensures city names are cleaned of any unwanted space characters.\n",
    "cleaned_df = cleaned_df.withColumn('city_name', trim(col('city_name')))\n",
    "\n",
    "# Filter cities by 'founded_year' to keep only reasonable years (between 1000 and 2025)\n",
    "# Removes cities with obviously invalid foundation years.\n",
    "cleaned_df = cleaned_df.filter((col('founded_year') >= 1000) & (col('founded_year') <= 2025))\n",
    "\n",
    "# Remove rows where 'state' or 'country' is missing (null)\n",
    "# Ensures all cities have both a state and a country specified.\n",
    "cleaned_df = cleaned_df.filter(col('state').isNotNull() & col('country').isNotNull())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
